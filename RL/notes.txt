carla setup:
https://www.youtube.com/playlist?list=PLQVvvaa0QuDeI12McNQdnTlWz9XlCa0uo

geohot - an architecture for life [https://geohot.github.io/blog/jekyll/update/2021/10/29/an-architecture-for-life.html]:
- freeze the weights of the representation, then train dynamics and prediction in a simulator
- temperature parameter = 0. Nobody would be happy if their car “explored”, it should only “exploit”

behavioral cloning [https://www.andrew.cmu.edu/course/10-403/slides/S19_lecture2_behaviorcloning.pdf]

lecture - reinforcement-intro.pdf:
- use policy gradient (43) => loss = log(π) * A (where A is the reward)

lecture - practical_deep_RL.pdf:
- policy gradients for trajectories (14)

Planning with Reinforcement Learning - [ https://www.youtube.com/watch?v=T39xkKN7uwo ]

Policy with Continuous Actions - [ https://medium.com/geekculture/policy-based-methods-for-a-continuous-action-space-7b5ecffac43a ]

Actor-Critic Reinforcement for continuous actions! - [ https://www.youtube.com/watch?v=Wj_5usZyb0M ]

general:
- observation (o) comes from CNN latent space + desire => state (s)
- temporal perceptions comes from RNN latent space
- action (a) comes from policy network (π)
- a = best_trajectory => steering angle

loss = -sum(R(τ)) (just optimize best trajectory)
loss = sum(log(best_traj_prob) * R(τ)) (for regression + classification (?))

other resources:
- https://huggingface.co/blog/deep-rl-pg

